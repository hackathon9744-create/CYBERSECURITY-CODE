import os
import joblib
import json
import pandas as pd
import gdown


from ai_engine.features import extract_features
from ai_engine.llm_client import llm_analyze_mock, llm_analyze_openai

# ===========================================================
# LOAD TRAINED MODEL
# ===========================================================
MODEL_PATH = "phishing_detection_random_forest_model_sklearn.joblib"
DRIVE_FILE_ID = "1LAr4G_-dD2viga5nH_Y8Ki_QwJhd32gD"
if not os.path.exists(MODEL_PATH):
  print("Downloading large url model from google drive")
  url = f"https://drive.google.com/uc?id={DRIVE_FILE_ID}"
  gdown.download(url,MODEL_PATH,quiet=False)
clf = joblib.load(MODEL_PATH)

# ===========================================================
# SCORE WEIGHTS
# ===========================================================
WEIGHTS = {
    "model": 0.45,
    "llm": 0.30,
    "struct": 0.25,
}

# ===========================================================
# STRUCTURAL SCORING
# ===========================================================
def structural_score(features):
    score = 0
    reasons = []

    if features["brand_sim"] > 0.70:
        score += 30
        reasons.append("Brand similarity indicates impersonation.")

    if features["homoglyph_ratio"] > 0.02:
        score += 30
        reasons.append("Homoglyph attack detected.")

    if features["suspicious_path_token"]:
        score += 20
        reasons.append("Suspicious path tokens (login/verify/otp).")

    if features["length"] > 80:
        score += 10
        reasons.append("URL is unusually long.")

    return min(score, 100), reasons


# ===========================================================
# MAIN ANALYSIS PIPELINE
# ===========================================================
def analyze_url_pipeline(url, fetch_page=False, use_openai=False):

    # Step 1 → Extract features
    feats = extract_features(url, fetch_page)

    # Step 2 → Prepare ML input
    # Get the feature names the model was trained on
    model_expected_features = clf.feature_names_in_

    # Create a Series with all expected features, initialized to 0.0
    input_series = pd.Series(0.0, index=model_expected_features)

    # Populate the series with features extracted by `extract_features` where they directly map
    # Note: Many features from the original dataset are not generated by extract_features.
    # They will remain 0.0, which might impact accuracy but resolves the `ValueError`.

    # Direct mappings/conversions based on feature names in the original dataset and extracted features
    if 'length_url' in input_series.index:
        input_series['length_url'] = feats.get('length', 0)
    if 'nb_hyphens' in input_series.index:
        input_series['nb_hyphens'] = feats.get('hyphen_count', 0)
    if 'domain_age' in input_series.index:
        input_series['domain_age'] = feats.get('whois_age_days', 0)
    if 'https_token' in input_series.index:
        input_series['https_token'] = int(feats.get('ssl_valid', False)) # Convert boolean to int (0/1)
    if 'phish_hints' in input_series.index:
        input_series['phish_hints'] = feats.get('suspicious_path_token', 0)

    # Convert the Series to a DataFrame with a single row
    X = pd.DataFrame([input_series])

    # Step 3 → ML prediction
    model_proba = clf.predict_proba(X)[0]
    classes = list(clf.classes_)

    if "malicious" in classes:
        mal_index = classes.index("malicious")
        model_score = float(model_proba[mal_index])
    else:
        model_score = float(model_proba[1]) # Assuming 1 is phishing/malicious

    # Step 4 → Structural scoring
    struct_s, struct_reasons = structural_score(feats)
    struct_norm = struct_s / 100

    # Step 5 → LLM scoring
    llm_input = {
        "url": feats["url"],
        "host": feats["host"],
        "domain": feats["domain"],
        "brand_similarity": feats["brand_sim"],
        "homoglyph_ratio": feats["homoglyph_ratio"],
        "suspicious_tokens": bool(feats["suspicious_path_token"])
    }

    if use_openai and os.getenv("OPENAI_API_KEY"):
        llm_out = llm_analyze_openai(llm_input)
    else:
        llm_out = llm_analyze_mock(llm_input)

    llm_score = float(llm_out.get("confidence", 0.5))

    # Step 6 → FINAL SCORE FUSION
    final_score = (
        WEIGHTS["model"]  * model_score +
        WEIGHTS["llm"]    * llm_score +
        WEIGHTS["struct"] * struct_norm
    )

    final_score = max(0.0, min(1.0, final_score))

    # Final risk label
    if final_score >= 0.75:
        risk = "High"
    elif final_score >= 0.45:
        risk = "Suspicious"
    else:
        risk = "Low"

    # FINAL CLEAN OUTPUT STRUCTURE
    return {
        "url": url,
        "risk_level": risk,
        "final_score": round(final_score, 3),
        "model_malicious_probability": model_score,
        "structural_reasons": struct_reasons,
        "llm": llm_out,
        "features":feats
        }
